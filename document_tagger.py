#!/usr/bin/env python3
import os
import sys
import re
import csv
from pathlib import Path
from typing import List, Tuple, Dict
from collections import Counter

# å°è¯•å¯¼å…¥å¯é€‰ä¾èµ–
try:
    import docx
    HAS_DOCX = True
except ImportError:
    HAS_DOCX = False
    print("è­¦å‘Š: python-docx æœªå®‰è£…ï¼Œæ— æ³•å¤„ç†.docxæ–‡ä»¶")

try:
    import jieba
    import jieba.analyse
    HAS_JIEBA = True
except ImportError:
    HAS_JIEBA = False
    print("è­¦å‘Š: jieba æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•çš„å…³é”®è¯æå–")

class DocumentTagger:
    def __init__(self):
        # æ–‡æ¡£å±æ€§åˆ†ç±»å®šä¹‰
        self.document_types = {
            'éœ€æ±‚ç±»æ–‡æ¡£': [
                'éœ€æ±‚', 'åŠŸèƒ½éœ€æ±‚', 'ä¸šåŠ¡éœ€æ±‚', 'äº§å“éœ€æ±‚', 'PRD', 'éœ€æ±‚åˆ†æ', 
                'requirement', 'feature', 'ç”¨æˆ·éœ€æ±‚', 'åŠŸèƒ½è§„æ ¼', 'éœ€æ±‚è¯´æ˜'
            ],
            'æŠ€æœ¯ç±»æ–‡æ¡£': [
                'æŠ€æœ¯', 'å¼€å‘', 'ç¼–ç¨‹', 'ä»£ç ', 'å®ç°', 'æ¶æ„', 'è®¾è®¡', 'API',
                'development', 'coding', 'æŠ€æœ¯æ–¹æ¡ˆ', 'ç³»ç»Ÿè®¾è®¡', 'æ¥å£æ–‡æ¡£',
                'æ•°æ®åº“', 'ç®—æ³•', 'æ¡†æ¶', 'æŠ€æœ¯è§„èŒƒ'
            ],
            'æµ‹è¯•ç±»æ–‡æ¡£': [
                'æµ‹è¯•', 'è´¨é‡', 'QA', 'æµ‹è¯•ç”¨ä¾‹', 'æµ‹è¯•è®¡åˆ’', 'testing', 
                'æµ‹è¯•æŠ¥å‘Š', 'ç¼ºé™·', 'bug', 'è´¨é‡ä¿è¯', 'éªŒæ”¶æµ‹è¯•', 'æ€§èƒ½æµ‹è¯•'
            ],
            'è¿ç»´ç±»æ–‡æ¡£': [
                'è¿ç»´', 'éƒ¨ç½²', 'ç›‘æ§', 'æœåŠ¡å™¨', 'ç³»ç»Ÿè¿ç»´', 'devops', 
                'deploy', 'è¿ç»´æ‰‹å†Œ', 'æ•…éšœå¤„ç†', 'å¤‡ä»½', 'å®‰å…¨', 'é…ç½®ç®¡ç†'
            ],
            'çŸ¥è¯†ç±»æ–‡æ¡£': [
                'çŸ¥è¯†', 'æ•™ç¨‹', 'åŸ¹è®­', 'è¯´æ˜', 'æŒ‡å—', 'æ‰‹å†Œ', 'ä»‹ç»',
                'å­¦ä¹ ', 'åˆ†äº«', 'æ€»ç»“', 'ç»éªŒ', 'æœ€ä½³å®è·µ', 'è§„èŒƒ', 'æ ‡å‡†'
            ],
            'ç®¡ç†ç±»æ–‡æ¡£': [
                'ç®¡ç†', 'é¡¹ç›®ç®¡ç†', 'è®¡åˆ’', 'æµç¨‹', 'è§„èŒƒ', 'management', 
                'process', 'ä¼šè®®', 'å†³ç­–', 'æŠ¥å‘Š', 'æ€»ç»“', 'åˆ¶åº¦', 'æ”¿ç­–'
            ]
        }
        
        # é¡¹ç›®å…³é”®è¯æ˜ å°„ï¼ˆç”¨äºè¯†åˆ«æ‰€å±é¡¹ç›®ï¼‰
        self.project_keywords = {
            # ä¸šåŠ¡é¡¹ç›®
            'è¶…å“ä¸­å¿ƒé¡¹ç›®': ['è¶…å“', 'è¶…çº§å“ç‰Œ', 'å“ç‰Œä¸­å¿ƒ', 'å“ç‰Œè¿è¥'],
            'ç›´æ’­ä¸šåŠ¡é¡¹ç›®': ['ç›´æ’­', 'ä¸»æ’­', 'ç›´æ’­é—´', 'ç›´æ’­è¿è¥', 'ç›´æ’­å¹³å°'],
            'ç”µå•†ç³»ç»Ÿé¡¹ç›®': ['è®¢å•', 'å•†å“', 'è´­ç‰©è½¦', 'æ”¯ä»˜', 'ç‰©æµ', 'OMS'],
            'è´¢åŠ¡ç®¡ç†é¡¹ç›®': ['è´¢åŠ¡', 'ä¼šè®¡', 'æˆæœ¬', 'é¢„ç®—', 'æ”¶å…¥', 'æ”¯å‡º', 'ç»“ç®—'],
            'è¥é”€æ¨å¹¿é¡¹ç›®': ['è¥é”€', 'æ¨å¹¿', 'å¹¿å‘Š', 'æ´»åŠ¨', 'ç”¨æˆ·å¢é•¿', 'è½¬åŒ–'],
            'å†…å®¹è¿è¥é¡¹ç›®': ['å†…å®¹', 'åˆ›ä½œ', 'è§†é¢‘', 'å›¾ç‰‡', 'æ–‡ç« ', 'åˆ›æ„'],
            'ç”¨æˆ·è¿è¥é¡¹ç›®': ['ç”¨æˆ·è¿è¥', 'ç§åŸŸ', 'å®¢æœ', 'ç”¨æˆ·ç®¡ç†', 'å®¢æˆ·æœåŠ¡'],
            # æŠ€æœ¯é¡¹ç›®  
            'æŠ€æœ¯å¹³å°é¡¹ç›®': ['å¹³å°', 'ç³»ç»Ÿ', 'æ¶æ„', 'æŠ€æœ¯', 'å¼€å‘', 'äº§ç ”'],
            'æ•°æ®åˆ†æé¡¹ç›®': ['æ•°æ®', 'åˆ†æ', 'ç»Ÿè®¡', 'æŒ‡æ ‡', 'æŠ¥è¡¨', 'æ•°æ®åº“'],
            'ç§»åŠ¨ç«¯é¡¹ç›®': ['ç§»åŠ¨', 'APP', 'å°ç¨‹åº', 'ç§»åŠ¨ç«¯', 'iOS', 'Android'],
            'è¿ç»´ä¿éšœé¡¹ç›®': ['è¿ç»´', 'éƒ¨ç½²', 'ç›‘æ§', 'æœåŠ¡å™¨', 'è¿ç»´ä¿éšœ'],
            # ç®¡ç†é¡¹ç›®
            'äººåŠ›èµ„æºé¡¹ç›®': ['äººåŠ›', 'HR', 'æ‹›è˜', 'åŸ¹è®­', 'ç»©æ•ˆ', 'è–ªé…¬', 'å‘˜å·¥'],
            'è´¨é‡ç®¡æ§é¡¹ç›®': ['è´¨é‡', 'å“æ§', 'å“è´¨ç®¡ç†', 'è´¨é‡ä¿è¯', 'æµ‹è¯•'],
            'æµç¨‹ä¼˜åŒ–é¡¹ç›®': ['æµç¨‹', 'ä¼˜åŒ–', 'è§„èŒƒ', 'æ ‡å‡†åŒ–', 'åˆ¶åº¦', 'ç®¡ç†']
        }

    def extract_text_from_file(self, file_path: str) -> Tuple[str, str]:
        """ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬å’Œæ ‡é¢˜"""
        file_path = Path(file_path)
        title = file_path.stem
        text = ""
        
        if file_path.suffix.lower() == '.docx':
            if not HAS_DOCX:
                print("é”™è¯¯: éœ€è¦å®‰è£… python-docx æ¥å¤„ç† .docx æ–‡ä»¶")
                return title, ""
            try:
                doc = docx.Document(file_path)
                text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
            except Exception as e:
                print(f"è¯»å–Wordæ–‡æ¡£å¤±è´¥: {e}")
                return title, ""
        elif file_path.suffix.lower() == '.txt':
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
            except Exception as e:
                print(f"è¯»å–æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {e}")
                return title, ""
        else:
            print(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
            return title, ""
        
        return title, text

    def identify_project(self, text: str, title: str = "") -> str:
        """æ™ºèƒ½è¯†åˆ«æ‰€å±é¡¹ç›®ï¼Œä¼˜å…ˆä»æ–‡æ¡£æ ‡é¢˜å’Œå†…å®¹ä¸­ç›´æ¥æå–é¡¹ç›®åç§°"""
        text_lower = text.lower()
        title_lower = title.lower()
        combined_text = (title + " " + text).lower()
        
        # å¼ºåŒ–æ ‡é¢˜ä¼˜å…ˆçº§ - æ˜ç¡®é¡¹ç›®åç§°ä¼˜å…ˆäºé€šç”¨è¯æ±‡
        # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šæ˜ç¡®çš„é¡¹ç›®åç§°
        if any(word in title_lower for word in ['è¶…å“ä¸­å¿ƒ', 'è¶…å“']):
            return "è¶…å“ä¸­å¿ƒé¡¹ç›®"
        elif any(word in title_lower for word in ['è´¢åŠ¡', 'ç»“ç®—', 'ä¸šç»©', 'æ¯›åˆ©']):
            return "è´¢åŠ¡ç®¡ç†é¡¹ç›®"
        elif any(word in title_lower for word in ['æ–°äºº', 'å…¥èŒ', 'åŸ¹è®­']):
            return "äººåŠ›èµ„æºé¡¹ç›®"  
        elif any(word in title_lower for word in ['éƒ¨é—¨', 'ç»„ç»‡', 'å›¢é˜Ÿ', 'èŒè´£']):
            return "ç»„ç»‡ç®¡ç†é¡¹ç›®"
        
        # ç¬¬äºŒä¼˜å…ˆçº§ï¼šé¡¹ç›®ç›¸å…³ä¸šåŠ¡è¯æ±‡
        elif any(word in title_lower for word in ['å•†å®¶', 'æ˜Ÿé€‰', 'é¥æœ›æ˜Ÿé€‰']):
            return "è¶…å“ä¸­å¿ƒé¡¹ç›®"
        
        # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šé€šç”¨è¯æ±‡ï¼ˆä¼˜å…ˆçº§æœ€ä½ï¼‰
        elif any(word in title_lower for word in ['æ–‡æ¡£', 'åˆ†ç±»', 'æ”¶é›†', 'çŸ¥è¯†']):
            return "çŸ¥è¯†ç®¡ç†é¡¹ç›®"
        
        # ç¬¬ä¸€æ­¥ï¼šç›´æ¥ä»æ ‡é¢˜å’Œå†…å®¹ä¸­æå–æ˜ç¡®çš„é¡¹ç›®åç§°
        extracted_project = self._extract_explicit_project_name(title, text)
        if extracted_project:
            return extracted_project
        
        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨å…³é”®è¯åŒ¹é…ï¼ˆé™ä½æƒé‡ï¼Œæ›´çµæ´»ï¼‰
        project_scores = {}
        for project, keywords in self.project_keywords.items():
            score = 0
            for keyword in keywords:
                keyword_lower = keyword.lower()
                # æ–‡æ¡£å†…å®¹åŒ¹é…
                content_count = combined_text.count(keyword_lower)
                score += content_count
                
                # æ ‡é¢˜åŒ¹é…ç»™äºˆæ›´é«˜æƒé‡
                if keyword_lower in title_lower:
                    score += 3  # é™ä½æƒé‡ï¼Œé¿å…è¿‡åº¦ä¾èµ–é¢„è®¾åˆ†ç±»
                    
            if score > 0:
                project_scores[project] = score
        
        # ç¬¬ä¸‰æ­¥ï¼šå¦‚æœæœ‰é¢„è®¾åŒ¹é…ä½†åˆ†æ•°ä¸é«˜ï¼Œä¼˜å…ˆä½¿ç”¨æ™ºèƒ½æ¨æ–­
        if project_scores:
            max_score = max(project_scores.values())
            if max_score >= 3:  # åªæœ‰è¾ƒé«˜åŒ¹é…åº¦æ‰ä½¿ç”¨é¢„è®¾åˆ†ç±»
                best_project = max(project_scores, key=project_scores.get)
                return best_project
        
        # ç¬¬å››æ­¥ï¼šæ™ºèƒ½æ¨æ–­é¡¹ç›®åç§°
        return self._infer_project_from_content(combined_text, title)
    
    def _extract_explicit_project_name(self, title: str, text: str) -> str:
        """ä»æ ‡é¢˜å’Œå†…å®¹ä¸­ç›´æ¥æå–æ˜ç¡®çš„é¡¹ç›®åç§°"""
        import re
        
        # åˆå¹¶æ ‡é¢˜å’Œæ–‡æ¡£å¼€å¤´éƒ¨åˆ†ç”¨äºé¡¹ç›®åç§°æå–
        search_text = title + " " + text[:500]  # åªæœç´¢å‰500å­—ç¬¦ï¼Œæé«˜æ•ˆç‡
        
        # 1. ä»æ ‡é¢˜ä¸­æå–é¡¹ç›®åç§°æ¨¡å¼
        title_patterns = [
            r'(\w*ä¸­å¿ƒ)\s*[vV]?\d*\.?\d*',  # è¶…å“ä¸­å¿ƒV1.0
            r'(\w+ç³»ç»Ÿ)\s*[vV]?\d*\.?\d*',   # è´¢åŠ¡ç³»ç»ŸV1.0
            r'(\w+å¹³å°)\s*[vV]?\d*\.?\d*',   # è¥é”€å¹³å°V1.0
            r'(\w+é¡¹ç›®)\s*[vV]?\d*\.?\d*',   # ç”µå•†é¡¹ç›®V1.0
            r'(\w+ä¸šåŠ¡)\s*[vV]?\d*\.?\d*',   # ç›´æ’­ä¸šåŠ¡V1.0
        ]
        
        for pattern in title_patterns:
            match = re.search(pattern, title)
            if match:
                project_core = match.group(1)
                # å°†è¯†åˆ«åˆ°çš„é¡¹ç›®æ ¸å¿ƒè¯è½¬æ¢ä¸ºæ ‡å‡†é¡¹ç›®å
                return self._standardize_project_name(project_core)
        
        # 2. ä»å†…å®¹ä¸­æå–é¡¹ç›®å…³é”®ä¿¡æ¯
        content_patterns = [
            r'é¡¹ç›®[:ï¼š]?\s*([^\s\n,ï¼Œã€‚]{2,10})',
            r'æ‰€å±é¡¹ç›®[:ï¼š]?\s*([^\s\n,ï¼Œã€‚]{2,10})',
            r'([^\s]{2,8})\s*PRD',  # PRDå‰çš„é¡¹ç›®å
            r'([^\s\n]{2,10})\s*éœ€æ±‚æ–‡æ¡£',
        ]
        
        for pattern in content_patterns:
            matches = re.findall(pattern, search_text)
            if matches:
                # é€‰æ‹©æœ€å¯èƒ½çš„é¡¹ç›®åï¼Œè¿‡æ»¤æ‰æ— æ•ˆåŒ¹é…
                for match in matches:
                    match = match.strip()
                    if (len(match) >= 2 and not match.isdigit() and 
                        not match.startswith('ï¼š') and 
                        'è§£å†³' not in match):  # è¿‡æ»¤æ‰"ï¼šè§£å†³ç°é˜¶æ®µè´¢åŠ¡"è¿™æ ·çš„è¯¯åŒ¹é…
                        standardized = self._standardize_project_name(match)
                        if standardized != f"{match}é¡¹ç›®":  # å¦‚æœæœ‰æ˜ç¡®çš„æ ‡å‡†åŒ–æ˜ å°„æ‰è¿”å›
                            return standardized
        
        return ""
    
    def _standardize_project_name(self, project_core: str) -> str:
        """å°†æå–çš„é¡¹ç›®æ ¸å¿ƒè¯æ ‡å‡†åŒ–ä¸ºé¡¹ç›®åç§°"""
        # æ¸…ç†ç‰ˆæœ¬å·å’Œç‰¹æ®Šå­—ç¬¦
        import re
        project_core = re.sub(r'[vV]?\d+\.?\d*', '', project_core).strip()
        
        # æ ‡å‡†åŒ–æ˜ å°„
        standardization_map = {
            'è¶…å“ä¸­å¿ƒ': 'è¶…å“ä¸­å¿ƒé¡¹ç›®',
            'è¶…å“': 'è¶…å“ä¸­å¿ƒé¡¹ç›®', 
            'è´¢åŠ¡': 'è´¢åŠ¡ç®¡ç†é¡¹ç›®',
            'è´¢åŠ¡ç³»ç»Ÿ': 'è´¢åŠ¡ç®¡ç†é¡¹ç›®',
            'è´¢åŠ¡ä¸“é¡¹': 'è´¢åŠ¡ç®¡ç†é¡¹ç›®',
            'æ–°äºº': 'äººåŠ›èµ„æºé¡¹ç›®',
            'å…¥èŒ': 'äººåŠ›èµ„æºé¡¹ç›®',
            'éƒ¨é—¨': 'ç»„ç»‡ç®¡ç†é¡¹ç›®',
            'ç»„ç»‡': 'ç»„ç»‡ç®¡ç†é¡¹ç›®',
            'æ–‡æ¡£åˆ†ç±»': 'çŸ¥è¯†ç®¡ç†é¡¹ç›®',
            'æ–‡æ¡£': 'çŸ¥è¯†ç®¡ç†é¡¹ç›®',
            'çŸ¥è¯†': 'çŸ¥è¯†ç®¡ç†é¡¹ç›®',
            'å•†å®¶ç«¯': 'è¶…å“ä¸­å¿ƒé¡¹ç›®',
            'é¥æœ›æ˜Ÿé€‰': 'è¶…å“ä¸­å¿ƒé¡¹ç›®',
            'èƒŒæ™¯': 'æŠ€æœ¯å¹³å°é¡¹ç›®',  # ä¸´æ—¶è§£å†³èƒŒæ™¯è¯¯è¯†åˆ«é—®é¢˜
        }
        
        # æŸ¥æ‰¾åŒ¹é…çš„æ ‡å‡†åŒ–åç§°
        for key, standard_name in standardization_map.items():
            if key in project_core:
                return standard_name
        
        # å¦‚æœæ²¡æœ‰ç›´æ¥åŒ¹é…ï¼Œæ ¹æ®å…³é”®è¯æ„é€ é¡¹ç›®å
        if any(word in project_core for word in ['ä¸­å¿ƒ', 'å¹³å°', 'ç³»ç»Ÿ']):
            return f"{project_core}é¡¹ç›®"
        elif 'ç®¡ç†' in project_core:
            return f"{project_core}é¡¹ç›®"
        else:
            return f"{project_core}é¡¹ç›®"
    
    def _infer_project_from_content(self, text: str, title: str = "") -> str:
        """åŸºäºå†…å®¹å’Œæ ‡é¢˜æ™ºèƒ½æ¨æ–­é¡¹ç›®ç±»å‹"""
        # æå–é«˜é¢‘è¯æ±‡ç”¨äºåˆ†æ
        if HAS_JIEBA:
            keywords = jieba.analyse.extract_tags(text, topK=15, withWeight=False)
        else:
            # ç®€å•åˆ†è¯
            words = []
            for word in text.replace('ï¼Œ', ' ').replace('ã€‚', ' ').split():
                word = word.strip(' \t\n.,!?;:()[]{}"\'-')
                if 2 <= len(word) <= 8:
                    words.append(word)
            from collections import Counter
            word_count = Counter(words)
            keywords = [word for word, count in word_count.most_common(15)]
        
        # ä¼˜å…ˆæ£€æŸ¥æ ‡é¢˜ä¸­çš„é¡¹ç›®çº¿ç´¢
        title_lower = title.lower()
        
        # å¼ºåŒ–æ ‡é¢˜ä¼˜å…ˆçº§ - æ˜ç¡®é¡¹ç›®åç§°ä¼˜å…ˆäºé€šç”¨è¯æ±‡
        # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šæ˜ç¡®çš„é¡¹ç›®åç§°
        if any(word in title_lower for word in ['è¶…å“ä¸­å¿ƒ', 'è¶…å“']):
            return "è¶…å“ä¸­å¿ƒé¡¹ç›®"
        elif any(word in title_lower for word in ['è´¢åŠ¡', 'ç»“ç®—', 'ä¸šç»©', 'æ¯›åˆ©']):
            return "è´¢åŠ¡ç®¡ç†é¡¹ç›®"
        elif any(word in title_lower for word in ['æ–°äºº', 'å…¥èŒ', 'åŸ¹è®­', 'onboard']):
            return "äººåŠ›èµ„æºé¡¹ç›®"  
        elif any(word in title_lower for word in ['éƒ¨é—¨', 'ç»„ç»‡', 'å›¢é˜Ÿ', 'èŒè´£']):
            return "ç»„ç»‡ç®¡ç†é¡¹ç›®"
        # ç¬¬äºŒä¼˜å…ˆçº§ï¼šé¡¹ç›®ç›¸å…³ä¸šåŠ¡è¯æ±‡
        elif any(word in title_lower for word in ['å•†å®¶', 'æ˜Ÿé€‰', 'é¥æœ›æ˜Ÿé€‰']):
            return "è¶…å“ä¸­å¿ƒé¡¹ç›®"
        # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šé€šç”¨è¯æ±‡ï¼ˆä¼˜å…ˆçº§æœ€ä½ï¼‰
        elif any(word in title_lower for word in ['æ–‡æ¡£', 'åˆ†ç±»', 'æ”¶é›†', 'çŸ¥è¯†']):
            return "çŸ¥è¯†ç®¡ç†é¡¹ç›®"
        
        # åŸºäºå†…å®¹å…³é”®è¯è¿›è¡Œæ›´ç»†è‡´çš„åˆ†ç±»
        # å®šä¹‰æ›´ç²¾ç¡®çš„é¢†åŸŸå…³é”®è¯
        financial_words = ['è´¢åŠ¡', 'ç»“ç®—', 'ä¸šç»©', 'æ¯›åˆ©', 'æˆæœ¬', 'æ”¶å…¥', 'æ”¯å‡º', 'é¢„ç®—', 'ä¼šè®¡']
        hr_words = ['äººåŠ›', 'æ‹›è˜', 'åŸ¹è®­', 'å‘˜å·¥', 'å…¥èŒ', 'è–ªé…¬', 'ç»©æ•ˆ', 'è€ƒæ ¸']
        product_words = ['äº§å“', 'éœ€æ±‚', 'PRD', 'åŠŸèƒ½', 'ç”¨æˆ·', 'ä½“éªŒ', 'è®¾è®¡']
        tech_words = ['æŠ€æœ¯', 'å¼€å‘', 'ç³»ç»Ÿ', 'å¹³å°', 'æ¶æ„', 'æ•°æ®åº“', 'æ¥å£', 'API']
        operation_words = ['è¿è¥', 'è¥é”€', 'æ¨å¹¿', 'æ´»åŠ¨', 'è½¬åŒ–', 'æ¸ é“', 'å®¢æˆ·']
        management_words = ['ç®¡ç†', 'æµç¨‹', 'åˆ¶åº¦', 'è§„èŒƒ', 'ä¼˜åŒ–', 'å›¢é˜Ÿ', 'ç»„ç»‡']
        knowledge_words = ['æ–‡æ¡£', 'çŸ¥è¯†', 'åˆ†ç±»', 'æ”¶é›†', 'æ•´ç†', 'å½’æ¡£', 'æŒ‡å—']
        
        # è®¡ç®—å„é¢†åŸŸè¯æ±‡çš„åŒ¹é…åº¦
        domain_scores = {
            'è´¢åŠ¡ç®¡ç†é¡¹ç›®': sum(1 for kw in keywords if any(fw in kw for fw in financial_words)),
            'äººåŠ›èµ„æºé¡¹ç›®': sum(1 for kw in keywords if any(hw in kw for hw in hr_words)),
            'äº§å“ç ”å‘é¡¹ç›®': sum(1 for kw in keywords if any(pw in kw for pw in product_words)),
            'æŠ€æœ¯å¹³å°é¡¹ç›®': sum(1 for kw in keywords if any(tw in kw for tw in tech_words)),
            'è¿è¥æ¨å¹¿é¡¹ç›®': sum(1 for kw in keywords if any(ow in kw for ow in operation_words)),
            'ç»„ç»‡ç®¡ç†é¡¹ç›®': sum(1 for kw in keywords if any(mw in kw for mw in management_words)),
            'çŸ¥è¯†ç®¡ç†é¡¹ç›®': sum(1 for kw in keywords if any(kw_word in kw for kw_word in knowledge_words)),
        }
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„é¡¹ç›®ç±»å‹
        if domain_scores:
            max_score = max(domain_scores.values())
            if max_score > 0:
                best_domain = max(domain_scores, key=domain_scores.get)
                return best_domain
        
        # å¦‚æœæ²¡æœ‰æ˜ç¡®åŒ¹é…ï¼Œä½¿ç”¨é€šç”¨åˆ†ç±»é€»è¾‘
        business_count = sum(1 for kw in keywords if any(bw in kw for bw in ['ç”¨æˆ·', 'äº§å“', 'ä¸šåŠ¡', 'è¿è¥', 'è¥é”€', 'å•†ä¸š']))
        tech_count = sum(1 for kw in keywords if any(tw in kw for tw in ['ç³»ç»Ÿ', 'å¹³å°', 'æŠ€æœ¯', 'å¼€å‘', 'æ•°æ®', 'åŠŸèƒ½']))
        mgmt_count = sum(1 for kw in keywords if any(mw in kw for mw in ['ç®¡ç†', 'æµç¨‹', 'åˆ¶åº¦', 'åŸ¹è®­', 'å›¢é˜Ÿ']))
        
        if tech_count >= business_count and tech_count >= mgmt_count:
            return "æŠ€æœ¯å¹³å°é¡¹ç›®"
        elif business_count >= mgmt_count:
            return "ä¸šåŠ¡è¿è¥é¡¹ç›®"
        else:
            return "ç»„ç»‡ç®¡ç†é¡¹ç›®"

    def classify_document_type(self, text: str, title: str = "") -> str:
        """åˆ†ç±»æ–‡æ¡£å±æ€§ï¼šéœ€æ±‚ç±»ã€æŠ€æœ¯ç±»ã€æµ‹è¯•ç±»ã€è¿ç»´ç±»ã€çŸ¥è¯†ç±»ã€ç®¡ç†ç±»"""
        text_lower = text.lower()
        title_lower = title.lower()
        combined_text = (title + " " + text).lower()
        
        type_scores = {}
        
        # è®¡ç®—æ¯ç§æ–‡æ¡£ç±»å‹çš„åŒ¹é…åˆ†æ•°
        for doc_type, keywords in self.document_types.items():
            score = 0
            for keyword in keywords:
                keyword_lower = keyword.lower()
                # æ–‡æ¡£å†…å®¹åŒ¹é…
                content_count = combined_text.count(keyword_lower)
                score += content_count
                
                # æ ‡é¢˜åŒ¹é…ç»™äºˆæ›´é«˜æƒé‡
                if keyword_lower in title_lower:
                    score += 5
                    
            if score > 0:
                type_scores[doc_type] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„æ–‡æ¡£ç±»å‹
        if type_scores:
            best_type = max(type_scores, key=type_scores.get)
            return best_type
        else:
            return "çŸ¥è¯†ç±»æ–‡æ¡£"  # é»˜è®¤ç±»å‹

    def _get_keywords_count(self, text_length: int) -> int:
        """æ ¹æ®æ–‡æ¡£é•¿åº¦åŠ¨æ€ç¡®å®šå…³é”®è¯æ•°é‡ï¼Œä¸è®¾å›ºå®šä¸Šé™"""
        if text_length < 200:
            return 2
        elif text_length < 500:
            return 3
        elif text_length < 1000:
            return 5
        elif text_length < 2000:
            return 8
        elif text_length < 3000:
            return 10
        elif text_length < 5000:
            return 12
        elif text_length < 8000:
            return 15
        elif text_length < 12000:
            return 18
        elif text_length < 20000:
            return 22
        else:
            # å¯¹äºè¶…é•¿æ–‡æ¡£ï¼ŒæŒ‰æ¯1000å­—ç¬¦çº¦1ä¸ªå…³é”®è¯çš„æ¯”ä¾‹
            return min(int(text_length / 1000) + 3, 30)  # æœ€å¤š30ä¸ªï¼Œé¿å…è¿‡å¤š

    def extract_keywords(self, text: str) -> List[str]:
        """åªä»æ–‡æ¡£åŸæ–‡ä¸­æå–ç¡®å®å­˜åœ¨çš„å…³é”®è¯"""
        if not text.strip():
            return []
        
        # æ ¹æ®æ–‡æ¡£é•¿åº¦ç¡®å®šå…³é”®è¯æ•°é‡
        num_keywords = self._get_keywords_count(len(text))
        
        # ç›´æ¥ä»åŸæ–‡æå–å…³é”®è¯ï¼Œä¸ä¾èµ–jiebaå¯èƒ½äº§ç”Ÿçš„è™šå‡è¯æ±‡
        return self._extract_verified_keywords_from_text(text, num_keywords)
    
    def _extract_verified_keywords_from_text(self, text: str, num_keywords: int) -> List[str]:
        """ä»åŸæ–‡ä¸­æå–å¹¶éªŒè¯å…³é”®è¯ï¼Œå®Œå…¨é¿å…jiebaäº§ç”Ÿè™šå‡è¯æ±‡"""
        import re
        import string
        from collections import Counter
        
        # åªä½¿ç”¨ç›´æ¥æ–‡æœ¬åˆ†å‰²ï¼Œç»å¯¹ä¸ä½¿ç”¨jiebaï¼Œé¿å…è™šå‡è¯æ±‡
        # æ–¹æ³•1ï¼šæŒ‰ä¸­æ–‡æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼åˆ†å‰²
        text_clean = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘\[\]\{\}<>ã€Šã€‹ã€\s]+', ' ', text)
        words1 = text_clean.split()
        
        # æ–¹æ³•2ï¼šæŒ‰è‹±æ–‡æ ‡ç‚¹åˆ†å‰²æå–æ›´å¤šå¯èƒ½çš„è¯æ±‡
        text_clean2 = re.sub(r'[.,!?;:\"\'\(\)\[\]\{\}<>\s]+', ' ', text)
        words2 = text_clean2.split()
        
        # æ–¹æ³•3ï¼šæå–è¿ç»­çš„ä¸­æ–‡å­—ç¬¦ç»„åˆ
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,8}', text)
        
        # åˆå¹¶æ‰€æœ‰å€™é€‰è¯æ±‡ï¼Œä½†åªä½¿ç”¨ç›´æ¥åˆ†å‰²çš„ç»“æœ
        candidate_words = words1 + words2 + chinese_words
        
        # ä¸¥æ ¼è¿‡æ»¤å’ŒéªŒè¯
        word_count = Counter()
        
        for word in candidate_words:
            word = word.strip(string.punctuation + ' \t\n')
            
            # åŸºæœ¬è¿‡æ»¤æ¡ä»¶
            if (2 <= len(word) <= 8 and 
                not word.isdigit() and 
                word.strip() and
                not word.isspace() and
                not word.lower() in ['html', 'http', 'https', 'www']):  # æ’é™¤ç½‘é¡µç›¸å…³è¯æ±‡
                
                # ç»å¯¹ä¸¥æ ¼éªŒè¯ï¼šè¯¥è¯æ±‡å¿…é¡»å®Œæ•´å­˜åœ¨äºåŸæ–‡ä¸­
                if self._absolute_strict_verify_word_in_text(word, text):
                    word_count[word] += 1
        
        # æŒ‰è¯é¢‘æ’åºé€‰æ‹©å…³é”®è¯
        # æ’é™¤è¿‡äºå¸¸è§çš„è¯æ±‡
        common_words = {'çš„', 'äº†', 'åœ¨', 'å’Œ', 'ä¸', 'ä¸º', 'æ˜¯', 'æœ‰', 'åŠ', 'ç­‰', 'å¯ä»¥', 'è¿›è¡Œ', 'é€šè¿‡', 'æˆ–è€…', 'å¦‚æœ', 'ä½†æ˜¯', 'å› ä¸º', 'æ‰€ä»¥', 'è¿™ä¸ª', 'é‚£ä¸ª', 'æˆ‘ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬', 'å®ƒä»¬', 'ä¹‹å', 'ä¹‹å‰', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å“ªé‡Œ', 'ä»€ä¹ˆæ—¶å€™', 'ä¸ºä»€ä¹ˆ', 'ä¸€ä¸ª', 'ä¸€ç§', 'ä¸€äº›', 'æ‰€æœ‰', 'æ¯ä¸ª', 'æ‰€ä»¥', 'å› æ­¤', 'ç„¶å', 'ç°åœ¨', 'å·²ç»', 'ä»ç„¶', 'åªæ˜¯', 'ä¹Ÿæ˜¯', 'è¿˜æ˜¯', 'æˆ–æ˜¯', 'å°±æ˜¯', 'ä¸æ˜¯', 'æ²¡æœ‰', 'è¿™äº›', 'é‚£äº›'}
        
        # é€‰æ‹©æœ€æœ‰æ„ä¹‰çš„å…³é”®è¯
        result_keywords = []
        for word, count in word_count.most_common():
            if (word not in common_words and 
                count >= 1 and 
                len(result_keywords) < num_keywords):
                result_keywords.append(word)
        
        return result_keywords
    
    def _absolute_strict_verify_word_in_text(self, word: str, text: str) -> bool:
        """ç»å¯¹ä¸¥æ ¼éªŒè¯è¯æ±‡ç¡®å®åœ¨åŸæ–‡ä¸­å­˜åœ¨"""
        # å¿…é¡»å®Œå…¨åŒ¹é…å­˜åœ¨äºåŸæ–‡ä¸­ï¼Œä¸èƒ½æœ‰ä»»ä½•å·®å¼‚
        return word in text
    
    def _strict_verify_word_in_text(self, word: str, text: str) -> bool:
        """ä¸¥æ ¼éªŒè¯è¯æ±‡ç¡®å®åœ¨åŸæ–‡ä¸­å­˜åœ¨"""
        # å¿…é¡»å®Œå…¨åŒ¹é…å­˜åœ¨äºåŸæ–‡ä¸­
        return word in text
    
    def _verify_keyword_in_text(self, keyword: str, text: str) -> bool:
        """ä¸¥æ ¼éªŒè¯å…³é”®è¯ç¡®å®åœ¨æ–‡æ¡£ä¸­å­˜åœ¨"""
        import re
        
        # å¯¹äºä¸­æ–‡å…³é”®è¯ï¼Œç›´æ¥æ£€æŸ¥æ˜¯å¦åœ¨æ–‡æœ¬ä¸­
        if re.search(r'[\u4e00-\u9fff]', keyword):
            return keyword in text
        
        # å¯¹äºè‹±æ–‡å…³é”®è¯ï¼Œæ£€æŸ¥å®Œæ•´è¯åŒ¹é…
        pattern = r'\b' + re.escape(keyword) + r'\b'
        if re.search(pattern, text, re.IGNORECASE):
            return True
            
        # é¢å¤–æ£€æŸ¥ï¼šæ˜¯å¦ä½œä¸ºä¸­è‹±æ··åˆè¯çš„ä¸€éƒ¨åˆ†å­˜åœ¨
        return keyword.lower() in text.lower()
    
    def _extract_simple_keywords_from_text(self, text: str, num_keywords: int) -> List[str]:
        """ä»æ–‡æ¡£æ–‡æœ¬ä¸­ç›´æ¥æå–ç®€å•å…³é”®è¯"""
        # åˆ†è¯å¤„ç†
        import re
        import string
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å¹¶åˆ†è¯
        words = []
        # æŒ‰ä¸­æ–‡æ ‡ç‚¹å’Œç©ºæ ¼åˆ†å‰²
        text_clean = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘\s]+', ' ', text)
        word_candidates = text_clean.split()
        
        # ç»Ÿè®¡è¯é¢‘
        word_count = {}
        for word in word_candidates:
            word = word.strip(string.punctuation + ' \t\n')
            if (2 <= len(word) <= 8 and 
                not word.isdigit() and 
                word.strip()):
                word_count[word] = word_count.get(word, 0) + 1
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)
        
        # å–å‰Nä¸ªé«˜é¢‘è¯
        result = []
        for word, count in sorted_words:
            if count >= 1 and len(result) < num_keywords:  # è‡³å°‘å‡ºç°1æ¬¡
                result.append(word)
        
        return result
    
    def _simple_keyword_extraction(self, text: str, num_keywords: int = 5) -> List[str]:
        """ç®€å•çš„å…³é”®è¯æå–æ–¹æ³•ï¼ˆå½“jiebaä¸å¯ç”¨æ—¶ï¼‰"""
        # æ‰©å¤§å¸¸è§å…³é”®è¯åº“
        common_keywords = [
            # ä¸šåŠ¡ç±»
            'ç”¨æˆ·', 'äº§å“', 'åŠŸèƒ½', 'ç³»ç»Ÿ', 'å¹³å°', 'æœåŠ¡', 'ç®¡ç†', 'å¼€å‘', 'è®¾è®¡', 'æµ‹è¯•',
            'ä¼˜åŒ–', 'ä½“éªŒ', 'éœ€æ±‚', 'æ–¹æ¡ˆ', 'é¡¹ç›®', 'ä¸šåŠ¡', 'æ•°æ®', 'åˆ†æ', 'è¿è¥', 'è¥é”€',
            # æŠ€æœ¯ç±»
            'æŠ€æœ¯', 'æ¥å£', 'æ•°æ®åº“', 'å‰ç«¯', 'åç«¯', 'ç§»åŠ¨ç«¯', 'ç½‘ç«™', 'åº”ç”¨', 'è½¯ä»¶', 'æ¶æ„',
            # æµç¨‹ç±»
            'æµç¨‹', 'è§„èŒƒ', 'æ ‡å‡†', 'åˆ¶åº¦', 'æ”¿ç­–', 'åŸ¹è®­', 'è€ƒæ ¸', 'ç»©æ•ˆ', 'è´¨é‡', 'å®‰å…¨',
            # è´¢åŠ¡ç±»
            'è´¢åŠ¡', 'æˆæœ¬', 'é¢„ç®—', 'æ”¶å…¥', 'æ”¯å‡º', 'åˆ©æ¶¦', 'æŠ•èµ„', 'é£é™©', 'åˆè§„', 'å®¡è®¡',
            # è¿è¥ç±»
            'æ¨å¹¿', 'æ´»åŠ¨', 'æ¸ é“', 'å®¢æˆ·', 'å¸‚åœº', 'å“ç‰Œ', 'å†…å®¹', 'ç¤¾ç¾¤', 'è½¬åŒ–', 'ç•™å­˜'
        ]
        
        # ç»Ÿè®¡å…³é”®è¯é¢‘ç‡
        keyword_count = {}
        text_lower = text.lower()
        
        for keyword in common_keywords:
            count = text_lower.count(keyword)
            if count > 0:
                keyword_count[keyword] = count
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)
        result = [kw[0] for kw in sorted_keywords[:num_keywords]]
        
        # å¦‚æœå…³é”®è¯æ•°é‡ä¸è¶³ï¼Œä½¿ç”¨ç®€å•çš„å­—é¢‘ç»Ÿè®¡è¡¥å……
        if len(result) < num_keywords:
            # ç®€å•åˆ†è¯ï¼ˆæŒ‰æ ‡ç‚¹å’Œç©ºæ ¼ï¼‰
            import string
            words = []
            for word in text.replace('ï¼Œ', ' ').replace('ã€‚', ' ').replace('ã€', ' ').split():
                word = word.strip(string.punctuation + ' \t\n')
                if 2 <= len(word) <= 6 and not word.isdigit():
                    words.append(word)
            
            # ç»Ÿè®¡è¯é¢‘
            word_count = Counter(words)
            additional_words = [word for word, count in word_count.most_common(num_keywords*2) 
                             if word not in result]
            
            result.extend(additional_words[:num_keywords - len(result)])
        
        return result[:num_keywords]
    
    def generate_content_summary(self, text: str, title: str = "") -> str:
        """ç”Ÿæˆè¯¦ç»†çš„æ–‡æ¡£å†…å®¹æ¦‚è¿°ï¼Œä¸é™åˆ¶å­—æ•°"""
        if not text.strip():
            return "æ–‡æ¡£å†…å®¹ä¸ºç©º"
        
        # æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬
        paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
        
        # æå–å’Œç»„ç»‡å†…å®¹çš„ä¸åŒéƒ¨åˆ†
        summary_parts = []
        
        if HAS_JIEBA:
            # æå–æ–‡æ¡£ç»“æ„åŒ–ä¿¡æ¯
            background_info = []
            objective_info = []
            content_info = []
            solution_info = []
            result_info = []
            
            # å…³é”®è¯åŒ¹é…æ¨¡å¼
            background_patterns = ['èƒŒæ™¯', 'ç°çŠ¶', 'é—®é¢˜', 'æŒ‘æˆ˜', 'åŸå› ', 'æƒ…å†µ']
            objective_patterns = ['ç›®æ ‡', 'ç›®çš„', 'æœŸæœ›', 'é¢„æœŸ', 'è®¡åˆ’', 'è¦æ±‚']
            content_patterns = ['å†…å®¹', 'åŠŸèƒ½', 'ç‰¹ç‚¹', 'ç‰¹æ€§', 'åŒ…æ‹¬', 'åŒ…å«', 'å…·ä½“']
            solution_patterns = ['æ–¹æ¡ˆ', 'è§£å†³', 'å®ç°', 'å®æ–½', 'æ‰§è¡Œ', 'æ“ä½œ', 'æ­¥éª¤', 'æµç¨‹']
            result_patterns = ['ç»“æœ', 'æ•ˆæœ', 'æ”¶ç›Š', 'ä»·å€¼', 'æˆæœ', 'æ€»ç»“', 'ç»“è®º']
            
            # åˆ†ææ¯ä¸ªæ®µè½
            for para in paragraphs:
                para_lower = para.lower()
                
                # æ£€æŸ¥èƒŒæ™¯ä¿¡æ¯
                if any(pattern in para_lower for pattern in background_patterns):
                    if len(para) > 15:
                        background_info.append(para)
                
                # æ£€æŸ¥ç›®æ ‡ä¿¡æ¯
                elif any(pattern in para_lower for pattern in objective_patterns):
                    if len(para) > 15:
                        objective_info.append(para)
                
                # æ£€æŸ¥è§£å†³æ–¹æ¡ˆ
                elif any(pattern in para_lower for pattern in solution_patterns):
                    if len(para) > 15:
                        solution_info.append(para)
                
                # æ£€æŸ¥ç»“æœæ•ˆæœ
                elif any(pattern in para_lower for pattern in result_patterns):
                    if len(para) > 15:
                        result_info.append(para)
                
                # æ£€æŸ¥å†…å®¹æè¿°
                elif any(pattern in para_lower for pattern in content_patterns):
                    if len(para) > 15:
                        content_info.append(para)
            
            # å¦‚æœæ²¡æœ‰ç»“æ„åŒ–å†…å®¹ï¼Œæå–å…³é”®æ®µè½
            if not any([background_info, objective_info, content_info, solution_info, result_info]):
                # æå–åŒ…å«å…³é”®è¯çš„é‡è¦æ®µè½
                important_keywords = [
                    'é¡¹ç›®', 'ç³»ç»Ÿ', 'å¹³å°', 'åŠŸèƒ½', 'éœ€æ±‚', 'è®¾è®¡', 'å¼€å‘', 'æµ‹è¯•', 'è¿ç»´',
                    'ç®¡ç†', 'æµç¨‹', 'è§„èŒƒ', 'ä¼˜åŒ–', 'å‡çº§', 'æ”¹è¿›', 'å®ç°', 'æ”¯æŒ', 'æä¾›',
                    'ç”¨æˆ·', 'å®¢æˆ·', 'ä¸šåŠ¡', 'æœåŠ¡', 'äº§å“', 'æ–¹æ¡ˆ', 'è®¡åˆ’', 'ç›®æ ‡', 'æ•ˆæœ'
                ]
                
                scored_paragraphs = []
                for para in paragraphs[:15]:  # å¤„ç†å‰15ä¸ªæ®µè½
                    if len(para) > 30:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                        score = 0
                        para_lower = para.lower()
                        for keyword in important_keywords:
                            score += para_lower.count(keyword)
                        
                        if score > 0:
                            scored_paragraphs.append((para, score))
                
                # æŒ‰åˆ†æ•°æ’åºï¼Œå–é‡è¦æ®µè½
                scored_paragraphs.sort(key=lambda x: x[1], reverse=True)
                content_info = [para[0] for para in scored_paragraphs[:8]]  # å–å‰8ä¸ªé‡è¦æ®µè½
            
            # ç»„ç»‡æ¦‚è¿°å†…å®¹
            if background_info:
                summary_parts.append(f"ã€èƒŒæ™¯æƒ…å†µã€‘{' '.join(background_info[:2])}")
            
            if objective_info:
                summary_parts.append(f"ã€é¡¹ç›®ç›®æ ‡ã€‘{' '.join(objective_info[:2])}")
            
            if content_info:
                summary_parts.append(f"ã€ä¸»è¦å†…å®¹ã€‘{' '.join(content_info[:4])}")
            
            if solution_info:
                summary_parts.append(f"ã€å®æ–½æ–¹æ¡ˆã€‘{' '.join(solution_info[:3])}")
            
            if result_info:
                summary_parts.append(f"ã€é¢„æœŸæ•ˆæœã€‘{' '.join(result_info[:2])}")
            
        else:
            # ç®€å•æ–¹æ³•ï¼šæŒ‰æ®µè½æå–
            important_paragraphs = []
            for para in paragraphs[:10]:
                if len(para) > 30:
                    important_paragraphs.append(para)
            
            if important_paragraphs:
                summary_parts.append(f"ã€æ–‡æ¡£å†…å®¹ã€‘{' '.join(important_paragraphs[:5])}")
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°ç»“æ„åŒ–å†…å®¹ï¼Œä½¿ç”¨å…¨æ–‡æ¦‚æ‹¬
        if not summary_parts:
            # ä½¿ç”¨æ–‡æ¡£å¼€å¤´éƒ¨åˆ†
            content_preview = []
            for para in paragraphs[:6]:
                if len(para) > 20:
                    content_preview.append(para)
            
            if content_preview:
                summary_parts.append(f"ã€æ–‡æ¡£æ¦‚è¿°ã€‘æœ¬æ–‡æ¡£ä¸»è¦é˜è¿°äº†{title}çš„ç›¸å…³å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š{' '.join(content_preview)}")
            else:
                summary_parts.append(f"ã€æ–‡æ¡£æ¦‚è¿°ã€‘æœ¬æ–‡æ¡£æè¿°äº†{title}ç›¸å…³å†…å®¹ï¼š{text[:300]}...")
        
        # ç”Ÿæˆæœ€ç»ˆæ¦‚è¿°
        final_summary = " ".join(summary_parts)
        
        return final_summary
    

    def format_output(self, title: str, project: str, doc_type: str, keywords: List[str], summary: str) -> Dict[str, str]:
        """æ ¼å¼åŒ–è¾“å‡ºç»“æœä¸ºæ–°çš„CSVæ ¼å¼"""
        return {
            'æ–‡æ¡£æ ‡é¢˜': title,
            'æ‰€å±é¡¹ç›®': project,
            'æ–‡æ¡£å…³é”®è¯': f"{doc_type}; " + ('; '.join(keywords) if keywords else ''),
            'æ–‡æ¡£å†…å®¹æ¦‚è¿°': summary
        }

    def process_document(self, file_path: str) -> Dict[str, str]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        print(f"æ­£åœ¨å¤„ç†æ–‡æ¡£: {file_path}")
        
        # æå–æ–‡æœ¬å’Œæ ‡é¢˜
        title, text = self.extract_text_from_file(file_path)
        
        print(f"æ–‡æ¡£æ ‡é¢˜: {title}")
        print(f"æ–‡æ¡£å†…å®¹é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # å¤„ç†ç©ºæ–‡æ¡£
        if not text.strip():
            print("âš ï¸ æ–‡æ¡£å†…å®¹ä¸ºç©º")
            return self.format_output(title, 'æœªçŸ¥é¡¹ç›®', 'çŸ¥è¯†ç±»æ–‡æ¡£', [], 'æ–‡æ¡£å†…å®¹ä¸ºç©º')
        
        # æ–°çš„åˆ†ç±»ä½“ç³»
        project = self.identify_project(text, title)
        doc_type = self.classify_document_type(text, title)
        keywords = self.extract_keywords(text)
        summary = self.generate_content_summary(text, title)
        
        print(f"æ‰€å±é¡¹ç›®: {project}")
        print(f"æ–‡æ¡£å±æ€§: {doc_type}")
        print(f"å…³é”®è¯æ•°é‡: {len(keywords)}")
        print(f"å…³é”®è¯: {keywords}")
        print(f"å†…å®¹æ¦‚è¿°é•¿åº¦: {len(summary)} å­—ç¬¦")
        
        # æ ¼å¼åŒ–è¾“å‡º
        result = self.format_output(title, project, doc_type, keywords, summary)
        return result
    
    def process_directory(self, directory_path: str) -> List[Dict[str, str]]:
        """æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ–‡æ¡£"""
        directory = Path(directory_path)
        
        if not directory.exists():
            print(f"é”™è¯¯: ç›®å½•ä¸å­˜åœ¨ - {directory_path}")
            return []
        
        if not directory.is_dir():
            print(f"é”™è¯¯: è·¯å¾„ä¸æ˜¯ç›®å½• - {directory_path}")
            return []
        
        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        supported_extensions = ['.txt', '.docx']
        
        # è·å–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        files_to_process = []
        for ext in supported_extensions:
            files_to_process.extend(directory.glob(f'*{ext}'))
        
        if not files_to_process:
            print(f"åœ¨ç›®å½• {directory_path} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {supported_extensions}")
            return []
        
        # æŒ‰æ–‡ä»¶åæ’åº
        files_to_process.sort(key=lambda x: x.name)
        
        print(f"æ‰¾åˆ° {len(files_to_process)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
        print("æ”¯æŒçš„æ ¼å¼:", supported_extensions)
        print()
        
        results = []
        for i, file_path in enumerate(files_to_process, 1):
            print(f"\n{'='*60}")
            print(f"å¤„ç†ç¬¬ {i}/{len(files_to_process)} ä¸ªæ–‡ä»¶")
            print(f"{'='*60}")
            
            try:
                result = self.process_document(str(file_path))
                results.append(result)
                
                print(f"\nâœ… æ–‡ä»¶ {file_path.name} å¤„ç†å®Œæˆ")
                
            except Exception as e:
                error_result = {
                    'æ–‡æ¡£æ ‡é¢˜': file_path.stem,
                    'æ‰€å±é¡¹ç›®': 'å¤„ç†å¤±è´¥',
                    'æ–‡æ¡£å…³é”®è¯': 'é”™è¯¯',
                    'æ–‡æ¡£å†…å®¹æ¦‚è¿°': str(e)
                }
                results.append(error_result)
                print(f"\nâŒ æ–‡ä»¶ {file_path.name} å¤„ç†å¤±è´¥: {e}")
        
        return results

def save_results_to_csv(results: List[Dict[str, str]], output_file_path: str = "document_tags.csv"):
    """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶ï¼Œé¿å…é‡å¤"""
    output_file = Path(output_file_path)
    
    # æ–°çš„CSVåˆ—å
    fieldnames = ['æ–‡æ¡£æ ‡é¢˜', 'æ‰€å±é¡¹ç›®', 'æ–‡æ¡£å…³é”®è¯', 'æ–‡æ¡£å†…å®¹æ¦‚è¿°']
    
    # è¯»å–ç°æœ‰å†…å®¹ï¼Œé¿å…é‡å¤
    existing_results = set()
    if output_file.exists():
        try:
            with open(output_file, "r", encoding="utf-8", newline='') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦
                    identifier = f"{row.get('æ–‡æ¡£æ ‡é¢˜', '')}-{row.get('æ‰€å±é¡¹ç›®', '')}-{row.get('æ–‡æ¡£å…³é”®è¯', '')}"
                    existing_results.add(identifier)
        except Exception as e:
            print(f"è¯»å–ç°æœ‰CSVæ–‡ä»¶å¤±è´¥: {e}")
    
    # è¿‡æ»¤å‡ºæ–°ç»“æœ
    new_results = []
    for result in results:
        identifier = f"{result['æ–‡æ¡£æ ‡é¢˜']}-{result['æ‰€å±é¡¹ç›®']}-{result['æ–‡æ¡£å…³é”®è¯']}"
        if identifier not in existing_results:
            new_results.append(result)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å†™å…¥è¡¨å¤´
    write_header = False
    if not output_file.exists():
        write_header = True
    else:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰è¡¨å¤´
        try:
            with open(output_file, "r", encoding="utf-8", newline='') as f:
                first_line = f.readline().strip()
                if not first_line or first_line != ','.join(fieldnames):
                    write_header = True
        except:
            write_header = True
    
    # å†™å…¥ç»“æœ
    if new_results or write_header:
        if write_header and output_file.exists():
            # éœ€è¦é‡å†™æ–‡ä»¶ä»¥æ·»åŠ è¡¨å¤´
            # å…ˆè¯»å–æ‰€æœ‰ç°æœ‰æ•°æ®
            existing_data = []
            try:
                with open(output_file, "r", encoding="utf-8", newline='') as f:
                    reader = csv.reader(f)
                    for row in reader:
                        if row:  # è·³è¿‡ç©ºè¡Œ
                            existing_data.append({
                                'æ–‡æ¡£æ ‡é¢˜': row[0] if len(row) > 0 else '',
                                'æ‰€å±é¡¹ç›®': row[1] if len(row) > 1 else '',
                                'æ–‡æ¡£å…³é”®è¯': row[2] if len(row) > 2 else '',
                                'æ–‡æ¡£å†…å®¹æ¦‚è¿°': row[3] if len(row) > 3 else ''
                            })
            except:
                existing_data = []
            
            # é‡å†™æ–‡ä»¶ï¼ŒåŒ…å«è¡¨å¤´å’Œæ‰€æœ‰æ•°æ®
            with open(output_file, "w", encoding="utf-8", newline='') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for row in existing_data:
                    writer.writerow(row)
                for result in new_results:
                    writer.writerow(result)
        else:
            # æ­£å¸¸è¿½åŠ æ¨¡å¼
            file_mode = 'a' if output_file.exists() and not write_header else 'w'
            with open(output_file, file_mode, encoding="utf-8", newline='') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                
                # å¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼Œå†™å…¥è¡¨å¤´
                if write_header:
                    writer.writeheader()
                
                # å†™å…¥æ•°æ®
                for result in new_results:
                    writer.writerow(result)
        
        if new_results:
            print(f"\nâœ… {len(new_results)} ä¸ªæ–°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        else:
            print(f"\nâœ… CSVæ–‡ä»¶å·²åˆ›å»º: {output_file}")
    else:
        print(f"\nâš ï¸ æ‰€æœ‰ç»“æœéƒ½å·²å­˜åœ¨äº: {output_file}ï¼Œè·³è¿‡é‡å¤ä¿å­˜")
    
    print(f"\nğŸ“ æ€»å…±å¤„ç†: {len(results)} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“ æ–°å¢ç»“æœ: {len(new_results)} ä¸ª")
    print(f"ğŸ“ é‡å¤è·³è¿‡: {len(results) - len(new_results)} ä¸ª")

def main():
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python document_tagger.py <æ–‡æ¡£è·¯å¾„æˆ–ç›®å½•è·¯å¾„>")
        print("æ”¯æŒæ ¼å¼: .docx, .txt")
        print("è¾“å‡ºæ ¼å¼: CSVè¡¨æ ¼ (document_tags.csv)")
        print("ç¤ºä¾‹:")
        print("  å•ä¸ªæ–‡ä»¶: python document_tagger.py /path/to/document.docx")
        print("  æ‰¹é‡å¤„ç†: python document_tagger.py /path/to/documents/")
        print("  æ‰¹é‡å¤„ç†å½“å‰ç›®å½•ä¸‹çš„documentsæ–‡ä»¶å¤¹: python document_tagger.py documents")
        sys.exit(1)
    
    input_path = sys.argv[1]
    
    if not os.path.exists(input_path):
        print(f"é”™è¯¯: è·¯å¾„ä¸å­˜åœ¨ - {input_path}")
        sys.exit(1)
    
    # åˆ›å»ºæ ‡ç­¾å™¨å®ä¾‹
    tagger = DocumentTagger()
    
    try:
        if os.path.isdir(input_path):
            # å¤„ç†ç›®å½•
            print(f"ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç†ç›®å½•: {input_path}")
            print("="*60)
            
            results = tagger.process_directory(input_path)
            
            if results:
                print(f"\n{'='*60}")
                print("æ‰¹é‡å¤„ç†å®Œæˆ - CSVè¡¨æ ¼æ ¼å¼ç»“æœæ±‡æ€»:")
                print("="*60)
                print(f"{'åºå·':<4} {'æ–‡æ¡£æ ‡é¢˜':<20} {'æ‰€å±é¡¹ç›®':<15} {'æ–‡æ¡£ç±»å‹':<12} {'å…³é”®è¯æ•°é‡':<10}")
                print("-" * 80)
                for i, result in enumerate(results, 1):
                    keywords = result['æ–‡æ¡£å…³é”®è¯'].split('; ')[1:] if result['æ–‡æ¡£å…³é”®è¯'] else []
                    keyword_count = len(keywords)
                    doc_type = result['æ–‡æ¡£å…³é”®è¯'].split('; ')[0] if result['æ–‡æ¡£å…³é”®è¯'] else ''
                    print(f"{i:<4} {result['æ–‡æ¡£æ ‡é¢˜']:<20} {result['æ‰€å±é¡¹ç›®']:<15} {doc_type:<12} {keyword_count:<10}")
                print("="*60)
                
                # ä¿å­˜æ‰€æœ‰ç»“æœåˆ°CSV
                save_results_to_csv(results)
            else:
                print("æ²¡æœ‰æ–‡ä»¶è¢«å¤„ç†")
                
        else:
            # å¤„ç†å•ä¸ªæ–‡ä»¶
            result = tagger.process_document(input_path)
            
            print("\n" + "="*60)
            print("æ–‡æ¡£æ ‡ç­¾åˆ†ç±»ç»“æœ (CSVæ ¼å¼):")
            print("="*60)
            print(f"æ–‡æ¡£æ ‡é¢˜: {result['æ–‡æ¡£æ ‡é¢˜']}")
            print(f"æ‰€å±é¡¹ç›®: {result['æ‰€å±é¡¹ç›®']}")
            print(f"æ–‡æ¡£å…³é”®è¯: {result['æ–‡æ¡£å…³é”®è¯']}")
            print(f"å†…å®¹æ¦‚è¿°: {result['æ–‡æ¡£å†…å®¹æ¦‚è¿°'][:100]}..." if len(result['æ–‡æ¡£å†…å®¹æ¦‚è¿°']) > 100 else f"å†…å®¹æ¦‚è¿°: {result['æ–‡æ¡£å†…å®¹æ¦‚è¿°']}")
            print("="*60)
            
            # ä¿å­˜ç»“æœåˆ°CSV
            save_results_to_csv([result])
        
    except Exception as e:
        print(f"å¤„ç†æ—¶å‡ºé”™: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()